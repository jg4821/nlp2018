{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "import spacy\n",
    "import string\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle as pkl\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.feature_extraction import stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(files):\n",
    "    data = []\n",
    "    for name in files:\n",
    "        try:\n",
    "            with open(name) as f:\n",
    "                for line in f:\n",
    "                    data.append(line)\n",
    "        except IOError as exc:\n",
    "            if exc.errno != errno.EISDIR:\n",
    "                raise\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()+'/aclImdb/train/pos/*.txt'\n",
    "files = glob.glob(path)\n",
    "train_pos_data = load_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()+'/aclImdb/train/neg/*.txt'\n",
    "files = glob.glob(path)\n",
    "train_neg_data = load_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = os.getcwd()+'/aclImdb/test/pos/*.txt'\n",
    "files = glob.glob(path)\n",
    "test_pos_data = load_data(files)\n",
    "path = os.getcwd()+'/aclImdb/test/neg/*.txt'\n",
    "files = glob.glob(path)\n",
    "test_neg_data = load_data(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12500"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_pos_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_pos_data + test_neg_data\n",
    "test_targets = ([1] * len(test_pos_data)) + ([0]*len(test_neg_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_split = 10000\n",
    "train_data = train_pos_data[:train_split] + train_neg_data[:train_split]\n",
    "train_targets = ([1]*train_split) + ([0]*train_split)\n",
    "\n",
    "val_data = train_pos_data[train_split:] + train_neg_data[train_split:]\n",
    "val_targets = ([1]*(len(train_pos_data)-train_split)) + ([0]*(len(train_pos_data)-train_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"While it certainly wasn't the best movie I've ever seen, it was certainly worth the $8 (which can't be said for many movies these days.)<br /><br />This was a pleasant account of a true story, although many of the details of the real story were twisted for the movie, (ie, Billy Sunday's character was three or four people in the real story combined together.) Robert DeNiro was of course good, and Cuba Gooding, Jr., was also impressive.\""
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[103]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import stop_words\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "tokenizer = spacy.load('en_core_web_sm')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "# lowercase and remove punctuation\n",
    "def tokenize(tokens,lower_case,remove_punc,remove_stopwords):\n",
    "#     tokens = tokenizer(sent)\n",
    "#     print(lower_case,remove_punc,remove_stopwords)\n",
    "    tokens = [token.text for token in tokens]\n",
    "    if lower_case:\n",
    "        tokens = [token.lower() for token in tokens]\n",
    "    if remove_punc:\n",
    "        tokens = [token for token in tokens if (token not in punctuations)]\n",
    "    if remove_stopwords:\n",
    "        tokens = [token for token in tokens if (token not in stop_words.ENGLISH_STOP_WORDS)]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def tokenize_dataset(dataset,lower_case,remove_punc,remove_stopwords):\n",
    "    token_dataset = []\n",
    "    # we are keeping track of all tokens in dataset\n",
    "    # in order to create vocabulary later\n",
    "    all_tokens = []\n",
    "\n",
    "    for sample in tqdm_notebook(tokenizer.pipe(dataset, disable=['parser', 'tagger', 'ner'], batch_size=512, n_threads=1)):\n",
    "        tokens = tokenize(sample,lower_case,remove_punc,remove_stopwords)\n",
    "        token_dataset.append(tokens)\n",
    "        all_tokens += tokens\n",
    "\n",
    "    return token_dataset, all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8db294b0ab44c4b9526a2a220ae4b40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a39a5ccaf84bb2bfb6fad8f2057304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a64cfe7219541b48b1df44e74bbff61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data,False,False,False)\n",
    "pkl.dump(val_data_tokens, open(\"val_none.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data,False,False,False)\n",
    "pkl.dump(test_data_tokens, open(\"test_none.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data,False,False,False)\n",
    "pkl.dump(train_data_tokens, open(\"train_none.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_none.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c8c7d9a3b54c4b88becf91c80e4269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44b3925a50d84ffd9814dac3d18397e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3793c537de604b51879351a5b5f51312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data,True,False,False)\n",
    "pkl.dump(val_data_tokens, open(\"val_lower.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data,True,False,False)\n",
    "pkl.dump(test_data_tokens, open(\"test_lower.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data,True,False,False)\n",
    "pkl.dump(train_data_tokens, open(\"train_lower.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_lower.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71930698457b497f91bff31b50963f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing test data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b810075316947928fb52f0091c238db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokenizing train data\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f165e01258a647fd82e2f02c8a275f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_data_tokens, _ = tokenize_dataset(val_data,True,True,True)\n",
    "pkl.dump(val_data_tokens, open(\"val_lowerpunc_stopw.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_data_tokens, _ = tokenize_dataset(test_data,True,True,True)\n",
    "pkl.dump(test_data_tokens, open(\"test_lowerpunc_stopw.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_data_tokens, all_train_tokens = tokenize_dataset(train_data,True,True,True)\n",
    "pkl.dump(train_data_tokens, open(\"train_lowerpunc_stopw.p\", \"wb\"))\n",
    "pkl.dump(all_train_tokens, open(\"all_train_lowerpunc_stopw.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### build ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build n-gram, assume n >= 2\n",
    "import copy\n",
    "\n",
    "def build_ngram(n,token_dataset):\n",
    "    all_n_grams = []\n",
    "    ngram_dataset = copy.deepcopy(token_dataset)\n",
    "    for i in range(2,n+1):\n",
    "        for ind in range(0,len(token_dataset)):\n",
    "            t = token_dataset[ind]\n",
    "            for j in range(0,len(t)-(i-1)):\n",
    "                new_gram = \" \".join(t[j:j+i])\n",
    "                ngram_dataset[ind].append(new_gram)\n",
    "    for x in ngram_dataset:\n",
    "        all_n_grams += x\n",
    "    return ngram_dataset,all_n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing val data\n",
      "Tokenizing test data\n",
      "Tokenizing train data\n"
     ]
    }
   ],
   "source": [
    "n = 4\n",
    "# val set tokens\n",
    "print (\"Tokenizing val data\")\n",
    "val_ngram, _ = build_ngram(n,val_data_tokens)\n",
    "pkl.dump(val_ngram, open(\"val_4gram.p\", \"wb\"))\n",
    "\n",
    "# test set tokens\n",
    "print (\"Tokenizing test data\")\n",
    "test_ngram, _ = build_ngram(n,test_data_tokens)\n",
    "pkl.dump(test_ngram, open(\"test_4gram.p\", \"wb\"))\n",
    "\n",
    "# train set tokens\n",
    "print (\"Tokenizing train data\")\n",
    "train_ngram, all_train_ngram = build_ngram(n,train_data_tokens)\n",
    "pkl.dump(train_ngram, open(\"train_4gram.p\", \"wb\"))\n",
    "pkl.dump(all_train_ngram, open(\"all_train_4gram.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['there',\n",
       " 'are',\n",
       " 'enough',\n",
       " 'sad',\n",
       " 'stories',\n",
       " 'about',\n",
       " 'women',\n",
       " 'and',\n",
       " 'their',\n",
       " 'oppression',\n",
       " 'by',\n",
       " 'religious',\n",
       " 'political',\n",
       " 'and',\n",
       " 'societal',\n",
       " 'means',\n",
       " 'not',\n",
       " 'to',\n",
       " 'diminish',\n",
       " 'the',\n",
       " 'films',\n",
       " 'and',\n",
       " 'stories',\n",
       " 'about',\n",
       " 'genital',\n",
       " 'mutilation',\n",
       " 'and',\n",
       " 'reproductive',\n",
       " 'rights',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'wage',\n",
       " 'inequality',\n",
       " 'and',\n",
       " 'marginalization',\n",
       " 'in',\n",
       " 'society',\n",
       " 'all',\n",
       " 'in',\n",
       " 'the',\n",
       " 'name',\n",
       " 'of',\n",
       " 'allah',\n",
       " 'or',\n",
       " 'god',\n",
       " 'or',\n",
       " 'some',\n",
       " 'other',\n",
       " 'ridiculous',\n",
       " 'justification',\n",
       " 'but',\n",
       " 'sometimes',\n",
       " 'it',\n",
       " 'is',\n",
       " 'helpful',\n",
       " 'to',\n",
       " 'just',\n",
       " 'take',\n",
       " 'another',\n",
       " 'approach',\n",
       " 'and',\n",
       " 'shed',\n",
       " 'some',\n",
       " 'light',\n",
       " 'on',\n",
       " 'the',\n",
       " 'subject.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'setting',\n",
       " 'is',\n",
       " 'the',\n",
       " '2006',\n",
       " 'match',\n",
       " 'between',\n",
       " 'iran',\n",
       " 'and',\n",
       " 'bahrain',\n",
       " 'to',\n",
       " 'qualify',\n",
       " 'for',\n",
       " 'the',\n",
       " 'world',\n",
       " 'cup',\n",
       " 'passions',\n",
       " 'are',\n",
       " 'high',\n",
       " 'and',\n",
       " 'several',\n",
       " 'women',\n",
       " 'try',\n",
       " 'to',\n",
       " 'disguise',\n",
       " 'themselves',\n",
       " 'as',\n",
       " 'men',\n",
       " 'to',\n",
       " 'get',\n",
       " 'into',\n",
       " 'the',\n",
       " 'match.<br',\n",
       " '/><br',\n",
       " '/>the',\n",
       " 'women',\n",
       " 'who',\n",
       " 'were',\n",
       " 'caught',\n",
       " 'played',\n",
       " 'by',\n",
       " 'sima',\n",
       " 'mobarak',\n",
       " 'shahi',\n",
       " 'shayesteh',\n",
       " 'irani',\n",
       " 'ayda',\n",
       " 'sadeqi',\n",
       " 'golnaz',\n",
       " 'farmani',\n",
       " 'and',\n",
       " 'mahnaz',\n",
       " 'zabihi',\n",
       " 'and',\n",
       " 'detained',\n",
       " 'for',\n",
       " 'prosecution',\n",
       " 'provided',\n",
       " 'a',\n",
       " 'funny',\n",
       " 'and',\n",
       " 'illuminating',\n",
       " 'glimpse',\n",
       " 'into',\n",
       " 'the',\n",
       " 'customs',\n",
       " 'of',\n",
       " 'this',\n",
       " 'country',\n",
       " 'and',\n",
       " 'most',\n",
       " 'likely',\n",
       " 'all',\n",
       " 'muslim',\n",
       " 'countries',\n",
       " 'their',\n",
       " 'interaction',\n",
       " 'with',\n",
       " 'the',\n",
       " 'iranian',\n",
       " 'soldiers',\n",
       " 'who',\n",
       " 'were',\n",
       " 'guarding',\n",
       " 'and',\n",
       " 'transporting',\n",
       " 'them',\n",
       " 'both',\n",
       " 'city',\n",
       " 'and',\n",
       " 'villagers',\n",
       " 'and',\n",
       " 'the',\n",
       " 'father',\n",
       " 'who',\n",
       " 'was',\n",
       " 'looking',\n",
       " 'for',\n",
       " 'his',\n",
       " 'daughter',\n",
       " 'provided',\n",
       " 'some',\n",
       " 'hilarious',\n",
       " 'moments',\n",
       " 'as',\n",
       " 'we',\n",
       " 'thought',\n",
       " 'about',\n",
       " 'why',\n",
       " 'they',\n",
       " 'have',\n",
       " 'such',\n",
       " 'unwritten',\n",
       " 'rules.<br',\n",
       " '/><br',\n",
       " '/>it',\n",
       " 'is',\n",
       " 'mainly',\n",
       " 'about',\n",
       " 'a',\n",
       " 'paternalistic',\n",
       " 'society',\n",
       " 'that',\n",
       " 'feels',\n",
       " 'it',\n",
       " 'has',\n",
       " 'to',\n",
       " 'save',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'women',\n",
       " 'from',\n",
       " 'the',\n",
       " 'crude',\n",
       " 'behavior',\n",
       " 'of',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'men',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'educating',\n",
       " 'the',\n",
       " 'male',\n",
       " 'population',\n",
       " 'they',\n",
       " 'deny',\n",
       " 'privilege',\n",
       " 'and',\n",
       " 'rights',\n",
       " 'to',\n",
       " 'the',\n",
       " 'women.<br',\n",
       " '/><br',\n",
       " '/>seeing',\n",
       " 'the',\n",
       " 'changes',\n",
       " 'in',\n",
       " 'the',\n",
       " 'soldiers',\n",
       " 'responsible',\n",
       " 'and',\n",
       " 'the',\n",
       " 'reflection',\n",
       " 'of',\n",
       " 'iranian',\n",
       " 'society',\n",
       " 'it',\n",
       " 'is',\n",
       " 'nos',\n",
       " 'surprise',\n",
       " 'this',\n",
       " 'film',\n",
       " 'will',\n",
       " 'not',\n",
       " 'get',\n",
       " 'any',\n",
       " 'play',\n",
       " 'in',\n",
       " 'iran',\n",
       " 'but',\n",
       " 'jafar',\n",
       " 'panahi',\n",
       " 'has',\n",
       " 'a',\n",
       " 'winner',\n",
       " 'on',\n",
       " 'his',\n",
       " 'hands',\n",
       " 'for',\n",
       " 'those',\n",
       " 'able',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'there are',\n",
       " 'are enough',\n",
       " 'enough sad',\n",
       " 'sad stories',\n",
       " 'stories about',\n",
       " 'about women',\n",
       " 'women and',\n",
       " 'and their',\n",
       " 'their oppression',\n",
       " 'oppression by',\n",
       " 'by religious',\n",
       " 'religious political',\n",
       " 'political and',\n",
       " 'and societal',\n",
       " 'societal means',\n",
       " 'means not',\n",
       " 'not to',\n",
       " 'to diminish',\n",
       " 'diminish the',\n",
       " 'the films',\n",
       " 'films and',\n",
       " 'and stories',\n",
       " 'stories about',\n",
       " 'about genital',\n",
       " 'genital mutilation',\n",
       " 'mutilation and',\n",
       " 'and reproductive',\n",
       " 'reproductive rights',\n",
       " 'rights as',\n",
       " 'as well',\n",
       " 'well as',\n",
       " 'as wage',\n",
       " 'wage inequality',\n",
       " 'inequality and',\n",
       " 'and marginalization',\n",
       " 'marginalization in',\n",
       " 'in society',\n",
       " 'society all',\n",
       " 'all in',\n",
       " 'in the',\n",
       " 'the name',\n",
       " 'name of',\n",
       " 'of allah',\n",
       " 'allah or',\n",
       " 'or god',\n",
       " 'god or',\n",
       " 'or some',\n",
       " 'some other',\n",
       " 'other ridiculous',\n",
       " 'ridiculous justification',\n",
       " 'justification but',\n",
       " 'but sometimes',\n",
       " 'sometimes it',\n",
       " 'it is',\n",
       " 'is helpful',\n",
       " 'helpful to',\n",
       " 'to just',\n",
       " 'just take',\n",
       " 'take another',\n",
       " 'another approach',\n",
       " 'approach and',\n",
       " 'and shed',\n",
       " 'shed some',\n",
       " 'some light',\n",
       " 'light on',\n",
       " 'on the',\n",
       " 'the subject.<br',\n",
       " 'subject.<br /><br',\n",
       " '/><br />the',\n",
       " '/>the setting',\n",
       " 'setting is',\n",
       " 'is the',\n",
       " 'the 2006',\n",
       " '2006 match',\n",
       " 'match between',\n",
       " 'between iran',\n",
       " 'iran and',\n",
       " 'and bahrain',\n",
       " 'bahrain to',\n",
       " 'to qualify',\n",
       " 'qualify for',\n",
       " 'for the',\n",
       " 'the world',\n",
       " 'world cup',\n",
       " 'cup passions',\n",
       " 'passions are',\n",
       " 'are high',\n",
       " 'high and',\n",
       " 'and several',\n",
       " 'several women',\n",
       " 'women try',\n",
       " 'try to',\n",
       " 'to disguise',\n",
       " 'disguise themselves',\n",
       " 'themselves as',\n",
       " 'as men',\n",
       " 'men to',\n",
       " 'to get',\n",
       " 'get into',\n",
       " 'into the',\n",
       " 'the match.<br',\n",
       " 'match.<br /><br',\n",
       " '/><br />the',\n",
       " '/>the women',\n",
       " 'women who',\n",
       " 'who were',\n",
       " 'were caught',\n",
       " 'caught played',\n",
       " 'played by',\n",
       " 'by sima',\n",
       " 'sima mobarak',\n",
       " 'mobarak shahi',\n",
       " 'shahi shayesteh',\n",
       " 'shayesteh irani',\n",
       " 'irani ayda',\n",
       " 'ayda sadeqi',\n",
       " 'sadeqi golnaz',\n",
       " 'golnaz farmani',\n",
       " 'farmani and',\n",
       " 'and mahnaz',\n",
       " 'mahnaz zabihi',\n",
       " 'zabihi and',\n",
       " 'and detained',\n",
       " 'detained for',\n",
       " 'for prosecution',\n",
       " 'prosecution provided',\n",
       " 'provided a',\n",
       " 'a funny',\n",
       " 'funny and',\n",
       " 'and illuminating',\n",
       " 'illuminating glimpse',\n",
       " 'glimpse into',\n",
       " 'into the',\n",
       " 'the customs',\n",
       " 'customs of',\n",
       " 'of this',\n",
       " 'this country',\n",
       " 'country and',\n",
       " 'and most',\n",
       " 'most likely',\n",
       " 'likely all',\n",
       " 'all muslim',\n",
       " 'muslim countries',\n",
       " 'countries their',\n",
       " 'their interaction',\n",
       " 'interaction with',\n",
       " 'with the',\n",
       " 'the iranian',\n",
       " 'iranian soldiers',\n",
       " 'soldiers who',\n",
       " 'who were',\n",
       " 'were guarding',\n",
       " 'guarding and',\n",
       " 'and transporting',\n",
       " 'transporting them',\n",
       " 'them both',\n",
       " 'both city',\n",
       " 'city and',\n",
       " 'and villagers',\n",
       " 'villagers and',\n",
       " 'and the',\n",
       " 'the father',\n",
       " 'father who',\n",
       " 'who was',\n",
       " 'was looking',\n",
       " 'looking for',\n",
       " 'for his',\n",
       " 'his daughter',\n",
       " 'daughter provided',\n",
       " 'provided some',\n",
       " 'some hilarious',\n",
       " 'hilarious moments',\n",
       " 'moments as',\n",
       " 'as we',\n",
       " 'we thought',\n",
       " 'thought about',\n",
       " 'about why',\n",
       " 'why they',\n",
       " 'they have',\n",
       " 'have such',\n",
       " 'such unwritten',\n",
       " 'unwritten rules.<br',\n",
       " 'rules.<br /><br',\n",
       " '/><br />it',\n",
       " '/>it is',\n",
       " 'is mainly',\n",
       " 'mainly about',\n",
       " 'about a',\n",
       " 'a paternalistic',\n",
       " 'paternalistic society',\n",
       " 'society that',\n",
       " 'that feels',\n",
       " 'feels it',\n",
       " 'it has',\n",
       " 'has to',\n",
       " 'to save',\n",
       " 'save it',\n",
       " \"it 's\",\n",
       " \"'s women\",\n",
       " 'women from',\n",
       " 'from the',\n",
       " 'the crude',\n",
       " 'crude behavior',\n",
       " 'behavior of',\n",
       " 'of it',\n",
       " \"it 's\",\n",
       " \"'s men\",\n",
       " 'men rather',\n",
       " 'rather than',\n",
       " 'than educating',\n",
       " 'educating the',\n",
       " 'the male',\n",
       " 'male population',\n",
       " 'population they',\n",
       " 'they deny',\n",
       " 'deny privilege',\n",
       " 'privilege and',\n",
       " 'and rights',\n",
       " 'rights to',\n",
       " 'to the',\n",
       " 'the women.<br',\n",
       " 'women.<br /><br',\n",
       " '/><br />seeing',\n",
       " '/>seeing the',\n",
       " 'the changes',\n",
       " 'changes in',\n",
       " 'in the',\n",
       " 'the soldiers',\n",
       " 'soldiers responsible',\n",
       " 'responsible and',\n",
       " 'and the',\n",
       " 'the reflection',\n",
       " 'reflection of',\n",
       " 'of iranian',\n",
       " 'iranian society',\n",
       " 'society it',\n",
       " 'it is',\n",
       " 'is nos',\n",
       " 'nos surprise',\n",
       " 'surprise this',\n",
       " 'this film',\n",
       " 'film will',\n",
       " 'will not',\n",
       " 'not get',\n",
       " 'get any',\n",
       " 'any play',\n",
       " 'play in',\n",
       " 'in iran',\n",
       " 'iran but',\n",
       " 'but jafar',\n",
       " 'jafar panahi',\n",
       " 'panahi has',\n",
       " 'has a',\n",
       " 'a winner',\n",
       " 'winner on',\n",
       " 'on his',\n",
       " 'his hands',\n",
       " 'hands for',\n",
       " 'for those',\n",
       " 'those able',\n",
       " 'able to',\n",
       " 'to see',\n",
       " 'see it']"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_ngram[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_2gram = pkl.load(open(\"train_2gram.p\", \"rb\"))\n",
    "all_train_2gram = pkl.load(open(\"all_train_2gram.p\", \"rb\"))\n",
    "\n",
    "val_2gram = pkl.load(open(\"val_2gram.p\", \"rb\"))\n",
    "test_2gram = pkl.load(open(\"test_2gram.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load preprocessed train, val and test datasets\n",
    "train_data_tokens = pkl.load(open(\"train_data_tokens.p\", \"rb\"))\n",
    "all_train_tokens = pkl.load(open(\"all_train_tokens.p\", \"rb\"))\n",
    "\n",
    "val_data_tokens = pkl.load(open(\"val_data_tokens.p\", \"rb\"))\n",
    "test_data_tokens = pkl.load(open(\"test_data_tokens.p\", \"rb\"))\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_tokens)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_tokens)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_tokens)))\n",
    "\n",
    "print (\"Total number of tokens in train dataset is {}\".format(len(all_train_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# len(all_train_tokens)\n",
    "# len(train_data_tokens)\n",
    "# train_data_tokens[0]\n",
    "\n",
    "# plot train dataset\n",
    "train_lengths = []\n",
    "for i in range(0,len(train_data_tokens)):\n",
    "    train_lengths.append(len(train_data_tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = Counter(train_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.4106e+04, 4.2650e+03, 1.1380e+03, 4.0100e+02, 7.9000e+01,\n",
       "        5.0000e+00, 2.0000e+00, 3.0000e+00, 0.0000e+00, 1.0000e+00]),\n",
       " array([  10. ,  261.1,  512.2,  763.3, 1014.4, 1265.5, 1516.6, 1767.7,\n",
       "        2018.8, 2269.9, 2521. ]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAGfCAYAAACQvXnVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAAADE0UAQAAQ/v/Y3EYzkHdkY0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1008x504 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "fig = plt.figure(1,figsize = (14,7)) \n",
    "# plt.scatter(range(1,len(train_data_tokens)+1),train_lengths, s=3)\n",
    "plt.hist(sorted(train_lengths),bins=10)\n",
    "# plt.plot(np.linspace(-1,1,num=1006),sorted(count.values()))\n",
    "# plt.plot(count.keys(),count.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bincount,binedge = np.histogram(train_lengths,bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18000.0"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bincount)*.90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14106,  4265,  1138,   401,    79,     5,     2,     3,     0,\n",
       "           1])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bincount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18371"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(bincount[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  10. ,  261.1,  512.2,  763.3, 1014.4, 1265.5, 1516.6, 1767.7,\n",
       "       2018.8, 2269.9, 2521. ])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_counter = Counter(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_counter.keys())\n",
    "sorted(token_counter.values(),reverse=True)[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6277"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_counter['br']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to create the vocabulary of most common 10,000 tokens in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "max_vocab_size = 10000\n",
    "# save index 0 for pad and 1 for unk\n",
    "PAD_IDX = 0\n",
    "UNK_IDX = 1\n",
    "\n",
    "def build_vocab(all_tokens):\n",
    "    # Returns:\n",
    "    # id2token: list of tokens, where id2token[i] returns token that corresponds to token i\n",
    "    # token2id: dictionary where keys represent tokens and corresponding values represent indices\n",
    "    token_counter = Counter(all_tokens)\n",
    "    vocab, count = zip(*token_counter.most_common(max_vocab_size))\n",
    "    id2token = list(vocab)\n",
    "    token2id = dict(zip(vocab, range(2,2+len(vocab)))) \n",
    "    id2token = ['<pad>', '<unk>'] + id2token\n",
    "    token2id['<pad>'] = PAD_IDX \n",
    "    token2id['<unk>'] = UNK_IDX\n",
    "    return token2id, id2token\n",
    "\n",
    "token2id, id2token = build_vocab(all_train_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token id 7845 ; token hobgoblins\n",
      "Token hobgoblins; token id 7845\n"
     ]
    }
   ],
   "source": [
    "# Lets check the dictionary by loading random token from it\n",
    "\n",
    "random_token_id = random.randint(0, len(id2token)-1)\n",
    "random_token = id2token[random_token_id]\n",
    "\n",
    "print (\"Token id {} ; token {}\".format(random_token_id, id2token[random_token_id]))\n",
    "print (\"Token {}; token id {}\".format(random_token, token2id[random_token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size is 20000\n",
      "Val dataset size is 5000\n",
      "Test dataset size is 25000\n"
     ]
    }
   ],
   "source": [
    "# convert token to id in the dataset\n",
    "def token2index_dataset(tokens_data):\n",
    "    indices_data = []\n",
    "    for tokens in tokens_data:\n",
    "        index_list = [token2id[token] if token in token2id else UNK_IDX for token in tokens]\n",
    "        indices_data.append(index_list)\n",
    "    return indices_data\n",
    "\n",
    "train_data_indices = token2index_dataset(train_data_tokens)\n",
    "val_data_indices = token2index_dataset(val_data_tokens)\n",
    "test_data_indices = token2index_dataset(test_data_tokens)\n",
    "\n",
    "# double checking\n",
    "print (\"Train dataset size is {}\".format(len(train_data_indices)))\n",
    "print (\"Val dataset size is {}\".format(len(val_data_indices)))\n",
    "print (\"Test dataset size is {}\".format(len(test_data_indices)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to create PyTorch DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SENTENCE_LENGTH = 400\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class NewsGroupDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Class that represents a train/validation/test dataset that's readable for PyTorch\n",
    "    Note that this class inherits torch.utils.data.Dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_list, target_list):\n",
    "        \"\"\"\n",
    "        @param data_list: list of newsgroup tokens \n",
    "        @param target_list: list of newsgroup targets \n",
    "\n",
    "        \"\"\"\n",
    "        self.data_list = data_list\n",
    "        self.target_list = target_list\n",
    "        assert (len(self.data_list) == len(self.target_list))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "        \n",
    "    def __getitem__(self, key):\n",
    "        \"\"\"\n",
    "        Triggered when you call dataset[i]\n",
    "        \"\"\"\n",
    "        \n",
    "        token_idx = self.data_list[key][:MAX_SENTENCE_LENGTH]\n",
    "        label = self.target_list[key]\n",
    "        return [token_idx, len(token_idx), label]\n",
    "\n",
    "def newsgroup_collate_func(batch):\n",
    "    \"\"\"\n",
    "    Customized function for DataLoader that dynamically pads the batch so that all \n",
    "    data have the same length\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    length_list = []\n",
    "    #print(\"collate batch: \", batch[0][0])\n",
    "    #batch[0][0] = batch[0][0][:MAX_SENTENCE_LENGTH]\n",
    "    for datum in batch:\n",
    "        label_list.append(datum[2])\n",
    "        length_list.append(datum[1])\n",
    "    # padding\n",
    "    for datum in batch:\n",
    "        padded_vec = np.pad(np.array(datum[0]), \n",
    "                                pad_width=((0,MAX_SENTENCE_LENGTH-datum[1])), \n",
    "                                mode=\"constant\", constant_values=0)\n",
    "        data_list.append(padded_vec)\n",
    "    return [torch.from_numpy(np.array(data_list)), torch.LongTensor(length_list), torch.LongTensor(label_list)]\n",
    "\n",
    "# create pytorch dataloader\n",
    "#train_loader = NewsGroupDataset(train_data_indices, train_targets)\n",
    "#val_loader = NewsGroupDataset(val_data_indices, val_targets)\n",
    "#test_loader = NewsGroupDataset(test_data_indices, test_targets)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataset = NewsGroupDataset(train_data_indices, train_targets)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_dataset = NewsGroupDataset(val_data_indices, val_targets)\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = NewsGroupDataset(test_data_indices, test_targets)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           collate_fn=newsgroup_collate_func,\n",
    "                                           shuffle=False)\n",
    "\n",
    "# for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "#     print(data)\n",
    "#     print(lengths)\n",
    "#     print(labels)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   4, 1059, 1410,  ...,    0,    0,    0],\n",
      "        [  93,   62,   22,  ...,    3,  184,    6],\n",
      "        [ 256,   25,  115,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [  29,   44,  115,  ...,    0,    0,    0],\n",
      "        [  11,    7,    4,  ...,  727,   68, 1681],\n",
      "        [  10, 1568,   57,  ...,    8, 2683,   92]])\n",
      "tensor([290, 400, 297,  40, 237,  59, 400, 400, 400, 400,  71, 288, 224, 232,\n",
      "        351, 156, 400, 119, 264, 400, 400, 400, 117, 186, 156, 385, 136, 127,\n",
      "        145,  71, 400, 400])\n",
      "tensor([0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1,\n",
      "        0, 0, 1, 0, 0, 1, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "    print(data)\n",
    "    print(lengths)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will define Bag-of-Words model in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First import torch related libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BagOfWords(nn.Module):\n",
    "    \"\"\"\n",
    "    BagOfWords classification model\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, emb_dim):\n",
    "        \"\"\"\n",
    "        @param vocab_size: size of the vocabulary. \n",
    "        @param emb_dim: size of the word embedding\n",
    "        \"\"\"\n",
    "        super(BagOfWords, self).__init__()\n",
    "        # pay attention to padding_idx \n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.linear = nn.Linear(emb_dim,2)\n",
    "    \n",
    "    def forward(self, data, length):\n",
    "        \"\"\"\n",
    "        \n",
    "        @param data: matrix of size (batch_size, max_sentence_length). Each row in data represents a \n",
    "            review that is represented using n-gram index. Note that they are padded to have same length.\n",
    "        @param length: an int tensor of size (batch_size), which represents the non-trivial (excludes padding)\n",
    "            length of each sentences in the data.\n",
    "        \"\"\"\n",
    "        out = self.embed(data)\n",
    "        out = torch.sum(out, dim=1)\n",
    "        out /= length.view(length.size()[0],1).expand_as(out).float()\n",
    "     \n",
    "        # return logits\n",
    "        out = self.linear(out.float())\n",
    "        return out\n",
    "\n",
    "emb_dim = 100\n",
    "model = BagOfWords(len(id2token), emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1/2], Step: [101/625], Validation Acc: 81.8\n",
      "Epoch: [1/2], Step: [201/625], Validation Acc: 85.06\n",
      "Epoch: [1/2], Step: [301/625], Validation Acc: 87.0\n",
      "Epoch: [1/2], Step: [401/625], Validation Acc: 87.08\n",
      "Epoch: [1/2], Step: [501/625], Validation Acc: 88.68\n",
      "Epoch: [1/2], Step: [601/625], Validation Acc: 88.72\n",
      "Epoch: [2/2], Step: [101/625], Validation Acc: 88.34\n",
      "Epoch: [2/2], Step: [201/625], Validation Acc: 88.56\n",
      "Epoch: [2/2], Step: [301/625], Validation Acc: 88.14\n",
      "Epoch: [2/2], Step: [401/625], Validation Acc: 87.96\n",
      "Epoch: [2/2], Step: [501/625], Validation Acc: 88.3\n",
      "Epoch: [2/2], Step: [601/625], Validation Acc: 87.12\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_epochs = 2 # number epoch to train\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Function for testing the model\n",
    "def test_model(loader, model):\n",
    "    \"\"\"\n",
    "    Help function that tests the model's performance on a dataset\n",
    "    @param: loader - data loader for the dataset to test against\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    for data, lengths, labels in loader:\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        outputs = F.softmax(model(data_batch, length_batch), dim=1)\n",
    "        predicted = outputs.max(1, keepdim=True)[1]\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels.view_as(predicted)).sum().item()\n",
    "    return (100 * correct / total)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, lengths, labels) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        data_batch, length_batch, label_batch = data, lengths, labels\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data_batch, length_batch)\n",
    "        loss = criterion(outputs, label_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # validate every 100 iterations\n",
    "        if i > 0 and i % 100 == 0:\n",
    "            # validate\n",
    "            val_acc = test_model(val_loader, model)\n",
    "            print('Epoch: [{}/{}], Step: [{}/{}], Validation Acc: {}'.format( \n",
    "                       epoch+1, num_epochs, i+1, len(train_loader), val_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training for 2 epochs\n",
      "Val Acc 87.92\n",
      "Test Acc 86.392\n"
     ]
    }
   ],
   "source": [
    "print (\"After training for {} epochs\".format(num_epochs))\n",
    "print (\"Val Acc {}\".format(test_model(val_loader, model)))\n",
    "print (\"Test Acc {}\".format(test_model(test_loader, model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
